{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploritory Data Analysis\n",
    "\n",
    "This file will take all the EDA performed by the team and made to run self contained on files from the GitHub repo.\n",
    "\n",
    "### Contents\n",
    "1. High Level Look At The Datasets\n",
    "    1. Client Supplied Datasets For Temperature And Demand\n",
    "    2. Solar PV Data\n",
    "    3. Population Data\n",
    "    4. Rain Data\n",
    "2. Dataset Interactions\n",
    "    1. A Closer Look At Demand Over Time\n",
    "    2. Demand With Weather And Datetime Data\n",
    "    3. Multivariate Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MSTL' from 'statsmodels.tsa.seasonal' (C:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\seasonal.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdateutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01measter\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mea\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseasonal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MSTL\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_decomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CCA\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'MSTL' from 'statsmodels.tsa.seasonal' (C:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\seasonal.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import dateutil.easter as ea\n",
    "from statsmodels.tsa.seasonal import MSTL\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2 import robjects\n",
    "CCP = importr(\"CCP\")\n",
    "base = importr(\"base\")\n",
    "quant_pys = importr(\"QuantPsyc\")\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "%matplotlib inline\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Look At The Datasets\n",
    "### Client Supplied Datasets for Temperature and Demand\n",
    "Here we will load in the client supplied datasets for temperature and demand and have a look at the overall structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/raw/temperature_nsw.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load in client datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m temp_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/raw/temperature_nsw.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m temp_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(temp_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m demand_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw/totaldemand_nsw.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdemand\u001b[39m\u001b[38;5;124m'\u001b[39m], header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/raw/temperature_nsw.csv'"
     ]
    }
   ],
   "source": [
    "# Load in client datasets\n",
    "temp_df = pd.read_csv('../data/raw/temperature_nsw.csv', names=['datetime', 'location', 'temp'], header=0)\n",
    "temp_df['datetime'] = pd.to_datetime(temp_df['datetime'])\n",
    "demand_df = pd.read_csv('../data/raw/totaldemand_nsw.csv', names=['datetime', 'region', 'demand'], header=0)\n",
    "demand_df['datetime'] = pd.to_datetime(demand_df['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the temperature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of null cells: {temp_df[\"temp\"].isnull().sum()}')\n",
    "print(temp_df.head(-1))\n",
    "print(temp_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is likely some record or sensor issue causing -9999 values. Let's have a look at the boxplot to see if we need to remove any other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.boxplot('temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it is just the -9999 values that will need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df[temp_df['temp'] > -9999]\n",
    "# Recreate boxplot with problem values removed\n",
    "ax = temp_df.boxplot('temp')\n",
    "plt.title('Boxplot of Temperature')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.savefig('../images/exploratory/temperature-boxplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = temp_df.hist('temp')\n",
    "plt.title('Temperature Histogram')\n",
    "plt.xlabel('Temperature (°C)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('../images/exploratory/temperature-histogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature records seem to follow a fairly normal trend.\n",
    "Let's now have a look at the timings of when records were made to look for irregularities in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x=temp_df['datetime'].dt.minute)\n",
    "plt.title('Temperature Timing Histogram')\n",
    "plt.xlabel('Minute of Hour')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(np.arange(0,60,step=5))\n",
    "plt.savefig('../images/exploratory/temperature-timing-histogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there may be some irregularities in the way the data was recoded but for the purposes of our analysis, this should not raise any issues.\n",
    "\n",
    "Let's now have a similar look at the demand dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of null cells: {demand_df[\"demand\"].isnull().sum()}')\n",
    "print(demand_df.head(-1))\n",
    "print(demand_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = demand_df.boxplot('demand')\n",
    "plt.title('Boxplot of Demand')\n",
    "plt.ylabel('Demand (MW)')\n",
    "plt.savefig('../images/exploratory/demand-boxplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the base load in demand is fairly consistent around 4000kW, were as there is much more variation and larger peaks in the maximum demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = demand_df.hist('demand')\n",
    "plt.title('Demand Histogram')\n",
    "plt.xlabel('Demand (MW)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('../images/exploratory/demand-histogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows a similar skew to the data as the boxplot.\n",
    "Again, let's have a look at the timings of when data was collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x=demand_df['datetime'].dt.minute, bins=12)\n",
    "plt.xticks(np.arange(0,60,step=5))\n",
    "plt.ylim([110250, 110340])\n",
    "plt.title('Demand Timing Histogram')\n",
    "plt.xlabel('Minute of Hour')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('../images/exploratory/demand-timing-histogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see some irregularities in how data was recorded, with more records being made on the hour, though for the purposes of our analysis it should not matter.\n",
    "Let's have a quick look at how demand has changed year-on-year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_df['year'] = demand_df.datetime.dt.year\n",
    "ax = sns.boxplot(data=demand_df, x='year', y='demand')\n",
    "plt.title('Demand by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Demand (MW)')\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.savefig('../images/exploratory/demand-by-year.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there may be a downward trend in demand, though it is difficult to say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solar PV Data\n",
    "We are interested in how residential, or small-scale, solar panels affect demand in NSW. Let's bring in some data from the Clean Energy Regulator (CER) and have a look at it. The data is spread across numerous .xlsx files that have data for all states so it will need to be filtered and combined before we can look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress a waring from openpyxl\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl')\n",
    "# Load in data from first workbook as starting point for df\n",
    "# named solar08_df as we will only end up taking the 2008 data from this df\n",
    "solar08_df = pd.read_excel('../data/raw/Postcode data for small-scale installations 2009 - all data.xlsx',\n",
    "                           sheet_name='SGU-Solar', header=2)\n",
    "solar08_df.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for filtering for NSW postcodes only i.e. 2XXX\n",
    "def nsw_mask(df):\n",
    "    mask = (df['Small Unit Installation Postcode'] >= 2000) & (df['Small Unit Installation Postcode'] < 3000)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out unnecessary locations\n",
    "mask08 = nsw_mask(solar08_df)\n",
    "nsw08_df = solar08_df.loc[mask08]\n",
    "\n",
    "# Get the initial data from prior to 2008\n",
    "INITIAL_SOLARUNITS = nsw08_df.sum()['Previous Years (2001-2007) - Installations Quantity']\n",
    "INITIAL_SOLARPOWER = nsw08_df.sum()['Previous Years (2001-2007) - SGU Rated Output In kW']\n",
    "\n",
    "# Get total number of units added in usable format\n",
    "solar_units = nsw08_df.filter(regex='2008 - Installations Quantity', axis=1).sum()\n",
    "solar_units.index = ['Jan 2008', 'Feb 2008', 'Mar 2008', 'Apr 2008', 'May 2008', 'Jun 2008',\n",
    "                     'Jul 2008', 'Aug 2008', 'Sep 2008', 'Oct 2008', 'Nov 2008', 'Dec 2008']\n",
    "solar_units.index = pd.to_datetime(solar_units.index)\n",
    "\n",
    "# Do the same for power added\n",
    "solar_output = nsw08_df.filter(regex='2008 - SGU Rated Output In kW', axis=1).sum()\n",
    "solar_output.index = ['Jan 2008', 'Feb 2008', 'Mar 2008', 'Apr 2008', 'May 2008', 'Jun 2008',\n",
    "                      'Jul 2008', 'Aug 2008', 'Sep 2008', 'Oct 2008', 'Nov 2008', 'Dec 2008']\n",
    "solar_output.index = pd.to_datetime(solar_output.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data from other workbooks\n",
    "months = ['Jan ', 'Feb ', 'Mar ', 'Apr ', 'May ', 'Jun ',\n",
    "          'Jul ', 'Aug ', 'Sep ', 'Oct ', 'Nov ', 'Dec ']\n",
    "\n",
    "# Loop through all the years we have solar data for and add the data to df\n",
    "for year in range(2009, 2021):\n",
    "    df = pd.read_excel('../data/raw/Postcode data for small-scale installations ' + str(year) + ' - all data.xlsx',\n",
    "                       sheet_name=0, header=2)\n",
    "    # filter by NSW data only\n",
    "    loc_mask = nsw_mask(df)\n",
    "    df = df.loc[loc_mask]\n",
    "    # separate Quantity and Output\n",
    "    df_units = df.filter(regex= str(year) + ' - Installations Quantity', axis=1).sum()\n",
    "    df_output = df.filter(regex= str(year) + ' - SGU Rated Output In kW', axis=1).sum()\n",
    "    # Reset index to datetime\n",
    "    month_year = []\n",
    "    for month in months:\n",
    "        month_year.append(month + str(year))\n",
    "    df_units.index = month_year\n",
    "    df_units.index = pd.to_datetime(df_units.index)\n",
    "    df_output.index = month_year\n",
    "    df_output.index = pd.to_datetime(df_output.index)\n",
    "    # Append to end of dataframe\n",
    "    solar_units = pd.concat([solar_units, df_units])\n",
    "    solar_output = pd.concat([solar_output, df_output])\n",
    "\n",
    "# Build usable dataset for export\n",
    "cum_solar_units = solar_units.cumsum() + INITIAL_SOLARUNITS\n",
    "cum_solar_output = solar_output.cumsum() + INITIAL_SOLARPOWER\n",
    "\n",
    "solar_df = pd.concat([solar_units, cum_solar_units, solar_output, cum_solar_output], axis=1)\n",
    "solar_df.rename(columns={0:'solar units added', 1:'cumulative units added', 2:'solar output added', 3:'cumulative output added'}, inplace=True)\n",
    "solar_df.index.names = ['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = solar_df['solar units added'].plot(ylabel='Number of Units Added', legend=True)\n",
    "solar_df['solar output added'].plot(ax=ax, secondary_y=True, ylabel='Output Added (kW)', legend=True)\n",
    "plt.title('Number of Units and Energy Output Added From Residential Solar Panels in NSW Each Month')\n",
    "plt.xlabel('Year')\n",
    "plt.savefig('../images/exploratory/units-output-plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of units being added in the 2010-2012 period is quite high, which may be due to government schemes, rebates, or even some change in they way the recording was being done in that time. As we will most likely be interested in the cumulative effects of solar, this will likely have little affect on our modelling.\n",
    "It is also worth noting that the Output being added per unit is increasing, meaning the technology is getting more efficient.\n",
    "\n",
    "Now let's look at the cumulative values that we are more interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = solar_df['cumulative units added'].plot(ylabel='Cumulative Units Added', legend=True)\n",
    "solar_df['cumulative output added'].plot(ax=ax, secondary_y=True, ylabel='Cumulative Output Added (kW)', legend=True)\n",
    "plt.title('Cumulative Units and Energy Output Added From Residential Solar Panels in NSW Each Month')\n",
    "plt.xlabel('Year')\n",
    "plt.savefig('../images/exploratory/cumulative-units-output-plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the large spike in units and output added in the 2010-2012 period is still noticeable, but less impactful in the cumulative output that we will likely be the most interested in.\n",
    "It's worth noting that this graph shows a similar trend in output growing faster than the units being added, but both look like the are growing at an increasing rate since the large addition in the 2010-2012 period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population Data\n",
    "Population will likely play a role in the demand. Let's bring in population data from the Australian Bureau of Statistics (ABS). The ABS data goes from 2001-2021 and is yearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import population data\n",
    "pop_df = pd.read_csv('../data/raw/NSW_population.csv', usecols=['TIME_PERIOD: Time Period', 'OBS_VALUE'], header=0)\n",
    "pop_df.rename(columns={'TIME_PERIOD: Time Period':'datetime', 'OBS_VALUE':'population'}, inplace=True)\n",
    "pop_df['datetime'] = pd.to_datetime(pop_df['datetime'], format='%Y')\n",
    "pop_df.set_index('datetime', inplace=True)\n",
    "pop_df.head(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df['population'].plot(title='Population in NSW', xlabel='Year', ylabel='Population', ylim=[0, 9e6])\n",
    "plt.savefig('../images/exploratory/NSW-population.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that population grows at a relatively steady rate. It is worth noting that 2021 had negative growth compared to all previous years, which were closer to 1%. This may be due to a correction to estimates from the census data. It will need to be considered when modelling and future forecasting, but these are likely the best estimates available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rain Data\n",
    "As we are interested in the effects of residential solar on the grid demand, we would like to incorporate cloud coverage data. As we don't have that we will use daily precipitation data from weather stations in areas with a lot of solar output as a proxy to see if it has an effect on grid demand due to changing residential solar output.\n",
    "\n",
    "First, we will take some data from the Australian PV Institute to find regions by postcode that have high solar output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_pv_df = pd.read_csv('../data/raw/Postcode time series.csv')\n",
    "postcode_df_sum = postcode_pv_df.groupby('Postcode').agg('sum', numeric_only=True)\n",
    "ax = postcode_df_sum.sort_values('Capacity (kW)').plot(kind='barh', title='Capacity by Postcode Region', legend=False, xlabel='Capacity (kW)')\n",
    "plt.savefig('../images/exploratory/output-by-postcode.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, the 21XX postcodes have the highest PV installation capacity. From this [map](https://pv-map.apvi.org.au/historical#9/-33.9115/150.5814), we can see that 21XX postcodes are the suburbs near the Sydney CBD.\n",
    "\n",
    "Now, we find the [BoM weather stations](http://www.bom.gov.au/nsw/observations/sydneymap.shtml) that are located in the 21XX suburbs, which are:\n",
    "\n",
    "*   PARRAMATTA NORTH (MASONS DRIVE), 066124\n",
    "*   KIAMA (BOMBO HEADLAND), 068242\n",
    "*   SYDNEY OLYMPIC PARK AWS (ARCHERY CENTRE), 066212\n",
    "*   CANTERBURY RACECOURSE AWS, 066194\n",
    "*   HORSLEY PARK EQUESTRIAN CENTRE AWS, 067119\n",
    "*   PROSPECT RESERVOIR, 067019\n",
    "\n",
    "The daily rain data is then extracted from those stations ([link](http://www.bom.gov.au/climate/data/)), and then we extract the data from 01/01/2010 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_ids = ['066124', '068242', '066212', '066194', '067119', '067019']\n",
    "\n",
    "# Get all data from weather stations\n",
    "for n in range(len(weather_station_ids)):\n",
    "    file_name = 'IDCJAC0009_' + weather_station_ids[n] + '_1800_Data.csv'\n",
    "\n",
    "    # Create new dataframe\n",
    "    if n == 0:\n",
    "        bom_df = pd.read_csv('../data/raw/' + file_name)\n",
    "        bom_df = bom_df.drop(columns=['Product code', 'Period over which rainfall was measured (days)', 'Quality'])\n",
    "        bom_df = bom_df[bom_df['Year'] >= 2010]\n",
    "        bom_df['Rainfall amount (millimetres)'] = bom_df['Rainfall amount (millimetres)'].fillna(0)\n",
    "\n",
    "    # Add to existing dataframe\n",
    "    else:\n",
    "        df = pd.read_csv('../data/raw/' + file_name)\n",
    "        df = df.drop(columns=['Product code', 'Period over which rainfall was measured (days)', 'Quality'])\n",
    "        df = df[df['Year'] >= 2010]\n",
    "        df['Rainfall amount (millimetres)'] = df['Rainfall amount (millimetres)'].fillna(0)\n",
    "        bom_df = pd.concat([bom_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single datetime column\n",
    "bom_df['datetime'] = pd.to_datetime(bom_df['Year'].astype(str) + '-' + bom_df['Month'].astype(str) + '-' + bom_df['Day'].astype(str))\n",
    "bom_df['datetime'] = pd.to_datetime(bom_df['datetime'])\n",
    "# Get the total amount of rain for all weather stations for each day\n",
    "rain_df = bom_df.groupby([\"datetime\"])[\"Rainfall amount (millimetres)\"].sum().to_frame()\n",
    "rain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = rain_df.boxplot('Rainfall amount (millimetres)')\n",
    "plt.title('Boxplot of Rainfall')\n",
    "plt.ylabel('Rainfall Amount (mm)')\n",
    "plt.savefig('../images/exploratory/rain-boxplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = rain_df.hist('Rainfall amount (millimetres)', bins=50)\n",
    "plt.title('Rainfall Histogram')\n",
    "plt.xlabel('Rainfall Amount (mm)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('../images/exploratory/rain-histogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram and boxplot, we can see that this is a very skewed dataset. Most values are 0 or close to with some very large outliers. This data may not be very useful for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking At Dataset Interactions\n",
    "We are now going to combine the proposed datasets and look at the interactions between some of the potential predictors in the model. We are interested in looking at how daily maximum and minimum demand changes over time, this will require some interpolation of data that are on different time scales, namely the solar_df and pop_df.\n",
    "As these data change at a relatively steady rate over the timescales that we will be interpolating, we will assume that a simple linear interpolation will suffice.\n",
    "\n",
    "We will also add in some predictors based on the date to see if date factors influence demand, such as weekends and public holidays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get daily min and max demand\n",
    "demand_df = demand_df.resample('D', on='datetime')['demand'].agg(['min', 'max'])\n",
    "demand_df.rename(columns={'min':'demand_min', 'max':'demand_max'}, inplace=True)\n",
    "\n",
    "# Get daily min and max temperatures\n",
    "temp_df = temp_df.resample('D', on='datetime')['temp'].agg(['min', 'max'])\n",
    "temp_df.rename(columns={'min':'temp_min', 'max':'temp_max', 'mean':'temp_mean'}, inplace=True)\n",
    "\n",
    "# Interpolate solar data but remove the amount of units added in a month\n",
    "solar_df.drop(['solar units added', 'solar output added'], axis=1, inplace=True)\n",
    "solar_df.rename(columns={'cumulative units added': 'cum_units', 'cumulative output added': 'cum_output'}, inplace=True)\n",
    "solar_df = solar_df.resample('D', convention='end').interpolate(method='linear')\n",
    "\n",
    "# Interpolate population data\n",
    "pop_df = pop_df.resample('D', convention='start').interpolate(method='linear')\n",
    "\n",
    "# Rename rainfall for simplicity\n",
    "rain_df.rename(columns={'Rainfall amount (millimetres)': 'rain'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all data into one dataframe\n",
    "all_data = demand_df.merge(temp_df, on='datetime', how='outer')\n",
    "all_data = all_data.merge(solar_df, on='datetime', how='outer')\n",
    "all_data = all_data.merge(pop_df, on='datetime', how='outer')\n",
    "all_data = all_data.merge(rain_df, on='datetime', how='outer')\n",
    "all_data.sort_index(inplace=True)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have sections that have large chunks of data missing so they wil have to be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_mask = (all_data.index >= '2010-01-01') & (all_data.index < '2020-12-01')\n",
    "all_data = all_data[bounds_mask]\n",
    "all_data.head(-1)\n",
    "\n",
    "# There are still some dates with missing temperature\n",
    "all_data = all_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add in our date time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_predictors(dataframe):\n",
    "    \"\"\"Function to create datetime features from a datetime index\"\"\"\n",
    "    df = dataframe.copy()\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['month'] = df.index.month\n",
    "    df['year'] = df.index.year\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    df['dayofmonth'] = df.index.day\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def public_holiday_check(new_date_check):\n",
    "    \"\"\"Function for finding public holidays in NSW\"\"\"\n",
    "    new_date = pd.Timestamp(new_date_check)\n",
    "    # New Year and Australia Day\n",
    "    if new_date.month == 1:\n",
    "        if new_date.weekday() not in (5,6):\n",
    "            if new_date.day in (1,26):\n",
    "                return 1\n",
    "        elif new_date.weekday() == 5:\n",
    "            if new_date.day in (3,28):\n",
    "                return 1\n",
    "        else:\n",
    "            if new_date.day in (2,27):\n",
    "                return 1\n",
    "    if new_date.month == 3 & ea.easter(new_date.year).month ==3:\n",
    "        if new_date.day in range(ea.easter(new_date.year).day-2,ea.easter(new_date.year).day+1):\n",
    "            return 1\n",
    "    # Anzac day\n",
    "    if new_date.month == 4:\n",
    "        if new_date.weekday() not in (5,6):\n",
    "            if new_date.day == 25:\n",
    "                return 1\n",
    "        elif new_date.weekday() == 6:\n",
    "            if new_date.day == 26:\n",
    "                return 1\n",
    "        else:\n",
    "            if new_date.day == 27:\n",
    "                return 1\n",
    "        if ea.easter(new_date.year).month == 4:\n",
    "            if new_date.day in range(ea.easter(new_date.year).day-2,ea.easter(new_date.year).day+1):\n",
    "                return 1\n",
    "    # king/queen birthday\n",
    "    if new_date.month == 6 & new_date.day > 7 & new_date.day < 15 & new_date.weekday() == 0:\n",
    "        return 1\n",
    "    # Labour day\n",
    "    if new_date.month == 10 & new_date.day < 8 & new_date.weekday() == 0:\n",
    "        return 1\n",
    "    # Christmas and boxing day\n",
    "    if new_date.month == 12:\n",
    "        if new_date.weekday() not in (5,6):\n",
    "            if new_date.day in (25,26):\n",
    "                return 1\n",
    "        elif new_date.weekday() == 5:\n",
    "            if new_date.day in (27,28):\n",
    "                return 1\n",
    "        else:\n",
    "            if new_date.day in (26,27):\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in datetime predictors\n",
    "all_data = create_date_predictors(all_data)\n",
    "all_data['public_holiday'] = all_data.index.to_frame()['datetime'].apply(lambda x: public_holiday_check(x))\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we now have a dataframe with all our proposed predictors. Let's have a closer look at some of the interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Closer Look At Demand Over Time\n",
    "We saw previously that there may be a downward trend in demand, let's have a closer look at the demand when you are just looking at the minimum and maximum demand for a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = all_data['demand_min'].plot(figsize=(10,5), ms=1, lw=1, ylabel='Demand (MW)', title='Mininmum and Maximum Daily Demand for NSW')\n",
    "all_data['demand_max'].plot(ax=ax)\n",
    "plt.xlabel('Year')\n",
    "plt.legend(['Min Demand', 'Max Demand'], loc='upper right')\n",
    "plt.savefig('../images/exploratory/min-max-demand.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite difficult in this plot to see the overall trend in the data.\n",
    "We will now do a timeseries decomposition to remove the noise and look at the trend. We won't tune the decomposition too much as we just want to get an idea of the broad trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MSTL from the statsmodels package to do multiple decomposition\n",
    "mstl_max = MSTL(all_data['demand_max'], periods=[7, 365])\n",
    "res_max = mstl_max.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"figure\", figsize=(8, 6))\n",
    "plt.rc(\"font\", size=13)\n",
    "fig = res_max.plot()\n",
    "fig.savefig('../images/exploratory/max-decomp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the same with the min demand\n",
    "mstl_min = MSTL(all_data['demand_min'], periods=[7, 365])\n",
    "res_min = mstl_min.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = res_min.plot()\n",
    "fig.savefig('../images/exploratory/min-decomp.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does look like there is a downward trend in both minimum and maximum demand, especially in the 2011-2014 period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demand With Weather And Datetime Data\n",
    "Let's now see if there are relationships between weather, datetime, and demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, sharey=True, figsize=(10,5))\n",
    "sns.scatterplot(data=all_data, x='temp_max', y='demand_max', ax=ax[0])\n",
    "ax[0].set_xlabel('Temperature (°C)')\n",
    "ax[0].set_title('Maximum Temperature')\n",
    "sns.scatterplot(data=all_data, x='temp_min', y='demand_max', ax=ax[1])\n",
    "ax[1].set_xlabel('Temperature (°C)')\n",
    "ax[1].set_title('Minimum Temperature')\n",
    "ax[0].set_ylabel('Demand (MW)')\n",
    "fig.suptitle('Maximum Daily Demand Vs Daily Temperature')\n",
    "plt.savefig('../images/exploratory/max-demand-temp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, sharey=True, figsize=(10,5))\n",
    "sns.scatterplot(data=all_data, x='temp_max', y='demand_min', ax=ax[0])\n",
    "ax[0].set_xlabel('Temperature (°C)')\n",
    "ax[0].set_title('Maximum Temperature')\n",
    "sns.scatterplot(data=all_data, x='temp_min', y='demand_min', ax=ax[1])\n",
    "ax[1].set_xlabel('Temperature (°C)')\n",
    "ax[1].set_title('Minimum Temperature')\n",
    "ax[0].set_ylabel('Demand (MW)')\n",
    "fig.suptitle('Minimum Daily Demand Vs Daily Temperature')\n",
    "plt.savefig('../images/exploratory/min-demand-temp.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum demand plotted against maximum temperature seems to have the most structure in the data. Minimum daily temperature with both minimum and maximum demand appear to have similar amounts of structure to the data. Maximum temperature with minimum demand appears to have the least strong relationship.\n",
    "\n",
    "Now let's do the same plots but with colour for days of week and public holidays to see if we can account for some of the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, sharey=True, figsize=(10,5))\n",
    "sns.scatterplot(data=all_data, x='temp_max', y='demand_max', hue='dayofweek', ax=ax[0])\n",
    "ax[0].set_xlabel('Temperature (°C)')\n",
    "ax[0].set_title('Maximum Temperature')\n",
    "sns.scatterplot(data=all_data, x='temp_min', y='demand_max', hue='dayofweek', ax=ax[1], legend=False)\n",
    "ax[1].set_xlabel('Temperature (°C)')\n",
    "ax[1].set_title('Minimum Temperature')\n",
    "ax[0].set_ylabel('Demand (MW)')\n",
    "fig.suptitle('Maximum Daily Demand Vs Daily Temperature With Day Of Week')\n",
    "plt.savefig('../images/exploratory/max-demand-temp-dayweek.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, sharey=True, figsize=(10,5))\n",
    "sns.scatterplot(data=all_data, x='temp_max', y='demand_min', hue='dayofweek', ax=ax[0])\n",
    "ax[0].set_xlabel('Temperature (°C)')\n",
    "ax[0].set_title('Maximum Temperature')\n",
    "sns.scatterplot(data=all_data, x='temp_min', y='demand_min', hue='dayofweek', ax=ax[1])\n",
    "ax[1].set_xlabel('Temperature (°C)')\n",
    "ax[1].set_title('Minimum Temperature')\n",
    "ax[0].set_ylabel('Demand (MW)')\n",
    "fig.suptitle('Minimum Daily Demand Vs Daily Temperature With Day Of Week')\n",
    "plt.savefig('../images/exploratory/min-demand-temp-dayweek.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the maximum daily demand is highly dependent on the day of the week, with Saturdays (5) and Sundays (6) tending to have lower demand than weekdays (0-4).\n",
    "The minimum daily demand shows less dependency on the day of the week, but it still looks to be a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, sharey=True, figsize=(10,5))\n",
    "sns.scatterplot(data=all_data, x='temp_max', y='demand_max', hue='public_holiday', alpha=0.6, ax=ax[0])\n",
    "ax[0].set_xlabel('Temperature (°C)')\n",
    "ax[0].set_title('Maximum Temperature')\n",
    "sns.scatterplot(data=all_data, x='temp_min', y='demand_max', hue='public_holiday', alpha=0.6, ax=ax[1], legend=False)\n",
    "ax[1].set_xlabel('Temperature (°C)')\n",
    "ax[1].set_title('Minimum Temperature')\n",
    "ax[0].set_ylabel('Demand (MW)')\n",
    "fig.suptitle('Maximum Daily Demand Vs Daily Temperature With Public Holidays')\n",
    "plt.savefig('../images/exploratory/max-demand-temp-pubhol.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, sharey=True, figsize=(10,5))\n",
    "sns.scatterplot(data=all_data, x='temp_max', y='demand_min', hue='public_holiday', alpha=0.6, ax=ax[0])\n",
    "ax[0].set_xlabel('Temperature (°C)')\n",
    "ax[0].set_title('Maximum Temperature')\n",
    "sns.scatterplot(data=all_data, x='temp_min', y='demand_min', hue='public_holiday', alpha=0.6, ax=ax[1], legend=False)\n",
    "ax[1].set_xlabel('Temperature (°C)')\n",
    "ax[1].set_title('Minimum Temperature')\n",
    "ax[0].set_ylabel('Demand (MW)')\n",
    "fig.suptitle('Minimum Daily Demand Vs Daily Temperature With Public Holidays')\n",
    "plt.savefig('../images/exploratory/min-demand-temp-pubhol.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like Public Holidays have a similar effect as weekends.\n",
    "\n",
    "Now let's see if the rain data has an effect. There are a lot of days that have no rain, the structure might be more visible if we filter out days that got less than 5mm of rain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, sharey=True, figsize=(10,5))\n",
    "sns.scatterplot(data=all_data[all_data['rain'] > 5], x='temp_max', y='demand_max', hue='rain', ax=ax[0])\n",
    "ax[0].set_xlabel('Temperature (°C)')\n",
    "ax[0].set_title('Maximum Temperature')\n",
    "sns.scatterplot(data=all_data[all_data['rain'] > 5], x='temp_min', y='demand_max', hue='rain', ax=ax[1], legend=False)\n",
    "ax[1].set_xlabel('Temperature (°C)')\n",
    "ax[1].set_title('Minimum Temperature')\n",
    "ax[0].set_ylabel('Demand (MW)')\n",
    "fig.suptitle('Maximum Daily Demand Vs Daily Temperature With Rain')\n",
    "plt.savefig('../images/exploratory/max-demand-temp-rain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, sharey=True, figsize=(10,5))\n",
    "sns.scatterplot(data=all_data[all_data['rain'] > 5], x='temp_max', y='demand_min', hue='rain', ax=ax[0])\n",
    "ax[0].set_xlabel('Temperature (°C)')\n",
    "ax[0].set_title('Maximum Temperature')\n",
    "sns.scatterplot(data=all_data[all_data['rain'] > 5], x='temp_min', y='demand_min', hue='rain', ax=ax[1], legend=False)\n",
    "ax[1].set_xlabel('Temperature (°C)')\n",
    "ax[1].set_title('Minimum Temperature')\n",
    "ax[0].set_ylabel('Demand (MW)')\n",
    "fig.suptitle('Minimum Daily Demand Vs Daily Temperature With Rain')\n",
    "plt.savefig('../images/exploratory/min-demand-temp-rain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, sharey=False, figsize=(10,5))\n",
    "sns.scatterplot(data=all_data, x='rain', y='demand_max', ax=ax[0])\n",
    "sns.scatterplot(data=all_data, x='rain', y='demand_min', ax=ax[1])\n",
    "ax[0].set_xlabel('Rainfall (mm)')\n",
    "ax[0].set_title('Maximum Demand')\n",
    "ax[0].set_ylabel('Demand (MW)')\n",
    "ax[1].set_xlabel('Rainfall (mm)')\n",
    "ax[1].set_title('Minimum Demand')\n",
    "ax[1].set_ylabel('Demand (MW)')\n",
    "fig.suptitle('Demand and Rainfall')\n",
    "plt.savefig('../images/exploratory/demand-rain.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like rain does not influence demand very much and may not be a good proxy for cloud cover if cloud cover does influence grid load. It may be that data from more weather stations would be needed to approximate cloud coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Analysis\n",
    "Now, let's have a look at how some of the features are linked by looking at the Canonical Correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly scale the data for CCA\n",
    "scaler = StandardScaler()\n",
    "norm_data = scaler.fit_transform(all_data)\n",
    "# Split into response and predictor data\n",
    "y_data = norm_data[:,:2]\n",
    "x_data = norm_data[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_normality(norm_data,alpha = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical Correlation Analysis\n",
    "ca = CCA()\n",
    "X_c, Y_c = ca.fit_transform(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between the first canonical pair\n",
    "np.corrcoef(X_c[:,0], Y_c[:,0])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframe for plotting heat map of correlations\n",
    "ccX_df = pd.concat([all_data, pd.DataFrame({'CCX':X_c[:, 0], 'CCY':Y_c[:, 0]}, index=all_data.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between all fields , this will help analysis the relationship between the Transformed predictors and the responses\n",
    "corr_X_df = ccX_df.corr(method='pearson')\n",
    "plt.rc('font', size=10)\n",
    "plt.figure(figsize=(10,8))\n",
    "X_df_lt = corr_X_df.where(np.tril(np.ones(corr_X_df.shape)).astype(bool))\n",
    "sns.heatmap(X_df_lt, cmap='coolwarm', annot=True, fmt='.1g')\n",
    "plt.tight_layout()\n",
    "plt.title('Heat Map with Predictor Canonical Correlations')\n",
    "plt.savefig('../images/exploratory/heatmap-cca-preds.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there appears to be strong relationships between the temperature variables and the first canonical correlation, where as the date variables seem to be loading on the second canonical correlation; day of the week seems to have a strong influence, backing up what we saw earlier in the scatter plots. It seems that solar output and population may have some contributions to models, but it would be minor compared to temperature and datetime predictors.\n",
    "Rain data still looks like it has little impact and can likely be dropped, as can day of the month.\n",
    "\n",
    "It is also worth noting that there appears to be a high amount of correlation with some of our predictors, as is often the case with timeseries data. To mitigate this, cumulative units will be dropped. We may have to be mindful of how entangled population, year, and cumulative output are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = CCP.p_perm(x_data,y_data,type = \"Wilks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data that will not be needed for model building\n",
    "all_data.drop(['cum_units', 'rain', 'dayofmonth'], axis=1, inplace=True)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output our clean data for modelling\n",
    "all_data.to_csv('../data/processed/all_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
